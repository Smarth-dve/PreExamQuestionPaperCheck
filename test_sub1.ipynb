{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\python311\\lib\\site-packages (2.12.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: C:\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "from PyPDF2 import PdfFileReader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Read our document**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read our pdf for the manifesto using the `PdfFileReader()` function from the PyPDF2 which is a package for extracting document information such as **title, author, number of pages,....**, spliting documents page by page, merging page by page, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '2022-May-CE245.pdf'\n",
    "open_filename = open(filename, 'rb')\n",
    "\n",
    "ind_manifesto = PyPDF2.PdfFileReader(open_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the document informtion  ussing the `getDocumentInfo()` function and check the number of pages in our document using the `numPages()` function. There are various useful functions one can use to check other things. See online documentation:[PyPDF2](https://pythonhosted.org/PyPDF2/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Title': 'CHAROTAR UNIVERSITY OF SCIENCE & TECHNOLOGY CHANGA',\n",
       " '/Author': 'ECC',\n",
       " '/Creator': 'Microsoft® Word 2016',\n",
       " '/CreationDate': \"D:20220507133605+05'30'\",\n",
       " '/ModDate': \"D:20220507133605+05'30'\",\n",
       " '/Producer': 'Microsoft® Word 2016'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_manifesto.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pages = ind_manifesto.numPages\n",
    "total_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outputs of our two previous codes, we got the **title** of the document, what OS was used to type the document, when the document was created and modified. And we also got the total number of pages in our document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Lets extract the texts from the pdf file and print it**\n",
    "\n",
    "We will use a `textract` package to extract our texts from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textract in c:\\python311\\lib\\site-packages (1.6.5)\n",
      "Requirement already satisfied: argcomplete~=1.10.0 in c:\\python311\\lib\\site-packages (from textract) (1.10.3)\n",
      "Requirement already satisfied: beautifulsoup4~=4.8.0 in c:\\python311\\lib\\site-packages (from textract) (4.8.2)\n",
      "Requirement already satisfied: chardet==3.* in c:\\python311\\lib\\site-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: docx2txt~=0.8 in c:\\python311\\lib\\site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: extract-msg<=0.29.* in c:\\python311\\lib\\site-packages (from textract) (0.28.7)\n",
      "Requirement already satisfied: pdfminer.six==20191110 in c:\\python311\\lib\\site-packages (from textract) (20191110)\n",
      "Requirement already satisfied: python-pptx~=0.6.18 in c:\\python311\\lib\\site-packages (from textract) (0.6.21)\n",
      "Requirement already satisfied: six~=1.12.0 in c:\\python311\\lib\\site-packages (from textract) (1.12.0)\n",
      "Requirement already satisfied: SpeechRecognition~=3.8.1 in c:\\python311\\lib\\site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: xlrd~=1.2.0 in c:\\python311\\lib\\site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\python311\\lib\\site-packages (from pdfminer.six==20191110->textract) (3.16.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python311\\lib\\site-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\python311\\lib\\site-packages (from beautifulsoup4~=4.8.0->textract) (2.3.2.post1)\n",
      "Requirement already satisfied: imapclient==2.1.0 in c:\\python311\\lib\\site-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
      "Requirement already satisfied: olefile>=0.46 in c:\\python311\\lib\\site-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
      "Requirement already satisfied: tzlocal>=2.1 in c:\\python311\\lib\\site-packages (from extract-msg<=0.29.*->textract) (4.2)\n",
      "Requirement already satisfied: compressed-rtf>=1.0.6 in c:\\python311\\lib\\site-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
      "Requirement already satisfied: ebcdic>=1.1.1 in c:\\python311\\lib\\site-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\python311\\lib\\site-packages (from python-pptx~=0.6.18->textract) (4.9.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\python311\\lib\\site-packages (from python-pptx~=0.6.18->textract) (9.3.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\python311\\lib\\site-packages (from python-pptx~=0.6.18->textract) (3.0.3)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\python311\\lib\\site-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\python311\\lib\\site-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (2022.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: C:\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract   \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop throug all the pages in the document and extract the text from it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "text  = ''\n",
    "\n",
    "# Lets loop through, to read each page from the pdf file\n",
    "while(count < total_pages):\n",
    "    # Get the specified number of pages in the document\n",
    "    mani_page  = ind_manifesto.getPage(count)\n",
    "    # Process the next page\n",
    "    count += 1\n",
    "    # Extract the text from the page\n",
    "    text += mani_page.extractText()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Seat no………….  \n",
      "Page 1 of 4 \n",
      "  CHAROTAR UNI VERSITY OF SCIENCE & TECHNOLOGY  \n",
      "Fourth  Semester of B. Tech (CE/CSE ) Examination  \n",
      "May 2022  \n",
      "CE245 Data Structure s and Algorithm s \n",
      "Date: 06.05.2022 , Friday  Time: 10.00 a.m. To 1.00 p.m.                      Maximum Marks: 70  \n",
      "Instructions:  \n",
      "1. The question paper comprises  two sections.  \n",
      "2. Section I and II must be attempted in separate answer sheets.  \n",
      "3. Make suitable assumptions and draw neat figures wherever required.  \n",
      "4. Use of scientific calculator is allowed.  \n",
      " \n",
      "SECTION – I \n",
      "Q - 1  Answer the question s below.   \n",
      "(i) What is the complexity of r unning Quick sort on an array of s ize n, which is already \n",
      "sorted?  [01] \n",
      "(ii) State True or False:  In a linked list representation of the simple queue, insertion should \n",
      "be performed at end and deletion should be performed from beginning.  \n",
      " [01] \n",
      "(iii) Why is it possible to access elements of an array in random order?  [01] \n",
      "(iv) Given an array A[-3:3, 5:10], Find the address of A[0][8 ] if array elements are organized \n",
      "in Column -Major order. Base Address is 2 00. Each element takes 2 bytes for storage.  \n",
      " [02] \n",
      "   \n",
      "Q – 2(a) Convert the following expression into postfix form using stack: A*(B+C )*D-E. Evaluate \n",
      "the postfix form for A=4, B=8, C= ( -2),D=3,E=9.  [03] \n",
      "Q – 2(b) Answer the questions below.  (Any Three)  [12] \n",
      "(i) Given code deletes the node which doesn't follow the ascending order sequence from this \n",
      "linked list : 3>>>4>>>5>>>6>>>7>>>9>>>8>>>10.  \n",
      "Write the correc t statements for a1,a2,a3,a4 . \n",
      "node structure is defined using this code:  \n",
      "struct node  \n",
      "{ \n",
      "int data;  \n",
      "struct node * link;  \n",
      "} *head, *p;  \n",
      "Code to remove node is::  \n",
      "struct node *C= head ; \n",
      "while(C ->data <= a1 ) \n",
      "{ \n",
      "p=a2 ; \n",
      "C=a3 ; \n",
      "} a4; \n",
      "  \n",
      "Candidate Seat no………….  \n",
      "Page 2 of 4 \n",
      " (ii) Explain the recursion stack for tower of Hanoi for number of discs=3.   \n",
      "(iii) Explain different cases of overflow condition for Circular Queue. Also mention the \n",
      "condition to display the queue contents for every case.   \n",
      "(iv) Consider the structure to store the details of students who have registered for the Codechef \n",
      "contest.  \n",
      "struct codechef_contest  \n",
      "{  \n",
      "     int rollno;  \n",
      "   char firstname[20];  \n",
      "   char lastname[20];  \n",
      "}; \n",
      "Details are arranged in ascending order (not necessarily contiguous) by roll numbers. \n",
      "Mention the efficient approach to change the last name of a student with any given roll \n",
      "number. Show the steps with any example.    \n",
      "  \n",
      "Q – 3 Answer the questions below.  (Any Three)  [15] \n",
      "(i) \n",
      " Compare Bubble sort and Selection sort for the following example with respect to number \n",
      "of passes and complexity.  \n",
      "   5,6,8,9,10,45,67,89,90,99  \n",
      "  \n",
      "(ii) \n",
      " Consider the doubly linked list with data 34,78,65,67,39.  \n",
      "1. Write the steps to insert node with data 100 at end.  \n",
      "2. Write the steps to delete a node with data 67.  \n",
      "  \n",
      "(iii) Consider the two sorted list  T1: 5,8,11 and T2: 4,6,9,15,20,28. W rite steps to merge the \n",
      "T1 and T2  in ascending order .  \n",
      "(iv) Write steps to replace  recursion with a stack,  to find the factorial of number 6.   \n",
      "   \n",
      "SECTION – II \n",
      "Q - 4  Answer the questions below.   \n",
      "(i) What is the maximum number of edges in an acyclic undirected graph with  n vertices?  [01] \n",
      "(ii) What is the maximum height of any AVL -tree with 12 nodes?  [01] \n",
      "(iii) A simple Graph has 24 edges and degree of each vertex is 4. Find the number of vertices.  [01] \n",
      "(iv) Match the following:  \n",
      "Applications  Data Structure s \n",
      "A. Routing tables  \n",
      "B. Music Playlist with next and previous navigation  \n",
      "C. Function calling and return  \n",
      "D. Social Network  I. Graph  \n",
      "II. Tree \n",
      "III. Doubly Linked List  \n",
      "IV. Stack  \n",
      " [02] \n",
      "Candidate Seat no………….  \n",
      "Page 3 of 4 \n",
      " Q-5(a) Justify that h ashing is an efficient searching method compared to Linear search and Binary  \n",
      "search .  [03] \n",
      "Q – 5(b) Answer the questions below.  (Any Three)  [12] \n",
      "       (i) The keys 22, 38, 33, 12, 3, 43, 15 and 85 are inserted into an initially empty hash table of \n",
      "length 10 using open addressing with hash function h(k) = k mod 10 and linear probing. \n",
      "What is the resultant hash table  and number of collisions?   \n",
      "       (ii) Show the adjacency matrix representation of the following graph . Find the in degree and \n",
      "out degree of every node from adjacency matrix . \n",
      "  \n",
      "     (iii) Apply Depth First traversal on following graph. Mention the sequence of each visited node \n",
      "using stack. Assume that Starting vertex is A and v isit the vertices in alphabetical order.  \n",
      "  \n",
      "  (iv)  Define the following terminologies:  \n",
      "1. Complete Graph  \n",
      "2. Weighted Graph  3. Perfect Binary tree  \n",
      "4. Complete Binary tree  \n",
      "  \n",
      "\n",
      "Candidate Seat no………….  \n",
      "Page 4 of 4 \n",
      " Q – 6 Attempt the following: (Any Three)  [15] \n",
      "(i) Preorder traversal s of a binary search tree is  given below:  45,11,23,41,39,85,77,68,96 . \n",
      "Construc t a binary search tree and write down the post order traversal .  \n",
      "(ii) What is the advantage of AVL tree over  BST? Insert the following keys in an empty AVL \n",
      "tree. Balance the tree wherever required.  \n",
      "89,67,55,54,34,33,32,22,12    \n",
      "(iii) Draw the recursion tree for preorder traversal of a following binary tree:  \n",
      "   \n",
      "(iv) Find the level order traversal of a complete binary tree.  \n",
      "6,3,5,4,18,11,12,16,10,9  \n",
      "Apply the heap sort to arrange the numbers in ascending order.   \n",
      " \n",
      " \n",
      "*** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_file_name = '2022-May-CE245.pdf'\n",
    "\n",
    "\n",
    "#Open the file in binary mode for reading\n",
    "with open(pdf_file_name, 'rb') as pdf_file:\n",
    "    #Read the PDF file\n",
    "    pdf_reader = PdfFileReader(pdf_file)\n",
    "    #Get number of pages in the PDF file\n",
    "    page_nums = pdf_reader.numPages\n",
    "    #Iterate over each page number\n",
    "    for page_num in range(page_nums):\n",
    "        #Read the given PDF file page\n",
    "        page = pdf_reader.getPage(page_num)\n",
    "        #Extract text from the given PDF file page\n",
    "        text = page.extractText()\n",
    "        #Print text\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Seat no………….  \n",
      "Page 4 of 4 \n",
      " Q – 6 Attempt the following: (Any Three)  [15] \n",
      "(i) Preorder traversal s of a binary search tree is  given below:  45,11,23,41,39,85,77,68,96 . \n",
      "Construc t a binary search tree and write down the post order traversal .  \n",
      "(ii) What is the advantage of AVL tree over  BST? Insert the following keys in an empty AVL \n",
      "tree. Balance the tree wherever required.  \n",
      "89,67,55,54,34,33,32,22,12    \n",
      "(iii) Draw the recursion tree for preorder traversal of a following binary tree:  \n",
      "   \n",
      "(iv) Find the level order traversal of a complete binary tree.  \n",
      "6,3,5,4,18,11,12,16,10,9  \n",
      "Apply the heap sort to arrange the numbers in ascending order.   \n",
      " \n",
      " \n",
      "*** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `if` statement check if our document returned words from the loop above using the `extractText()` function. This is done since `PyPDF2` cannot read scanned documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text != ' ':\n",
    "    text = text\n",
    "    \n",
    "else:\n",
    "    textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above returns a false, then run the Optical Character Recognition (OCR) `textract` to convert scanned/image based Pdf files to text. See `textract` online documentaion: [textract](https://textract.readthedocs.io/en/stable/python_package.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out our texts to see what it contains which was converted to lower case using the `lower()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\python311\\lib\\site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: C:\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method lower of str object at 0x0000017C1D7C7150>\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def to_lower(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specll check the words\n",
    "    spell  = Speller(lang='en')\n",
    "    \n",
    "    texts = spell(text)\n",
    "    \n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
    "\n",
    "# lower_case = to_lower(text)\n",
    "print(text.lower)\n",
    "lower_case=text.lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method lower of str object at 0x0000017C1D7C7150>\n"
     ]
    }
   ],
   "source": [
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And clear from seeing the printed text, we only extracted texts from page 2 till last pace. The reason being is that, those pages that we did not extract text from are in image based and we failed to to do. **SOMEONE CAN PERHAPS HELP!!!!.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Clean our *to_lower_case* text variable and return it as a list of keywords.**\n",
    "\n",
    "From the printed text, it's apparent that our text contains unwanted characters such as spaces, punctuations `\\n` and so forth. \n",
    "\n",
    "Lets break our text phrases into individual words using `word_tokenize()` function from the Naturalge Toolkit (nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords, brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lower_case):\n",
    "    # split text phrases into words\n",
    "    words  = nltk.word_tokenize(lower_case)\n",
    "    \n",
    "    \n",
    "    # Create a list of all the punctuations we wish to remove\n",
    "    punctuations = ['.', ',', '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n",
    "    \n",
    "    \n",
    "    # Remove all the special characters\n",
    "    punctuations = re.sub(r'\\W', ' ', str(lower_case))\n",
    "    \n",
    "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
    "    stop_words  = stopwords.words('english')\n",
    "    \n",
    "    # Getting rid of all the words that contain numbers in them\n",
    "    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n",
    "    \n",
    "    # remove all single characters\n",
    "    \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Removing non-english characters\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    # Return keywords which are not in stop words \n",
    "    keywords = [word for word in words if not word in stop_words  and word in punctuations and  word in w_num]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Seat no………….  \n",
      "Page 4 of 4 \n",
      " Q – 6 Attempt the following: (Any Three)  [15] \n",
      "(i) Preorder traversal s of a binary search tree is  given below:  45,11,23,41,39,85,77,68,96 . \n",
      "Construc t a binary search tree and write down the post order traversal .  \n",
      "(ii) What is the advantage of AVL tree over  BST? Insert the following keys in an empty AVL \n",
      "tree. Balance the tree wherever required.  \n",
      "89,67,55,54,34,33,32,22,12    \n",
      "(iii) Draw the recursion tree for preorder traversal of a following binary tree:  \n",
      "   \n",
      "(iv) Find the level order traversal of a complete binary tree.  \n",
      "6,3,5,4,18,11,12,16,10,9  \n",
      "Apply the heap sort to arrange the numbers in ascending order.   \n",
      " \n",
      " \n",
      "*** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Lemmatize the words\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatized_word = [wordnet_lemmatizer.lemmatize(words) for words in clean_text(text)]\n",
    "\n",
    "# # lets print out the output from our function above and see how the data looks like\n",
    "# clean_data = ' '.join(lemmatized_word)\n",
    "# print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save our data into a dataframe so we can do our anal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Itula</th>\n",
       "      <td>Candidate Seat no………….  \\nPage 4 of 4 \\n Q – 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  script\n",
       "Itula  Candidate Seat no………….  \\nPage 4 of 4 \\n Q – 6..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([text])\n",
    "df.columns = ['script']\n",
    "df.index = ['Itula']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **5. Preprocess - Bag of Words model** \n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision [Wikipedia](https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/).\n",
    "\n",
    "\n",
    "It is mostly used to extract features from text for used in modelling, such as machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itula    Candidate Seat no………….  \\nPage 4 of 4 \\n Q – 6...\n",
      "Name: script, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "corpus = df.script\n",
    "vectorizer = HashingVectorizer(n_features=2**24)\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "data_vect = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applt the document-term matrix which is a mathematical matrix which decribes the frequency of words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>...</th>\n",
       "      <th>post</th>\n",
       "      <th>preorder</th>\n",
       "      <th>recursion</th>\n",
       "      <th>required</th>\n",
       "      <th>search</th>\n",
       "      <th>seat</th>\n",
       "      <th>sort</th>\n",
       "      <th>traversal</th>\n",
       "      <th>tree</th>\n",
       "      <th>write</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Itula</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10  11  12  15  16  18  22  23  32  33  ...  post  preorder  recursion  \\\n",
       "Itula   1   2   2   1   1   1   1   1   1   1  ...     1         2          1   \n",
       "\n",
       "       required  search  seat  sort  traversal  tree  write  \n",
       "Itula         1       2     1     1          4     8      1  \n",
       "\n",
       "[1 rows x 57 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "data_vect_feat = pd.DataFrame(data_vect.toarray(), columns=feature_names)\n",
    "data_vect_feat.index = df.index\n",
    "data_vect_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vector representations show the frequency of words used in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Itula\n",
      "10             1\n",
      "11             2\n",
      "12             2\n",
      "15             1\n",
      "16             1\n",
      "18             1\n",
      "22             1\n",
      "23             1\n",
      "32             1\n",
      "33             1\n",
      "34             1\n",
      "39             1\n",
      "41             1\n",
      "45             1\n",
      "54             1\n",
      "55             1\n",
      "67             1\n",
      "68             1\n",
      "77             1\n",
      "85             1\n",
      "89             1\n",
      "96             1\n",
      "advantage      1\n",
      "apply          1\n",
      "arrange        1\n",
      "ascending      1\n",
      "attempt        1\n",
      "avl            2\n",
      "balance        1\n",
      "binary         4\n",
      "bst            1\n",
      "candidate      1\n",
      "complete       1\n",
      "construc       1\n",
      "draw           1\n",
      "following      3\n",
      "given          1\n",
      "heap           1\n",
      "ii             1\n",
      "iii            1\n",
      "insert         1\n",
      "iv             1\n",
      "keys           1\n",
      "level          1\n",
      "numbers        1\n",
      "order          3\n",
      "page           1\n",
      "post           1\n",
      "preorder       2\n",
      "recursion      1\n",
      "required       1\n",
      "search         2\n",
      "seat           1\n",
      "sort           1\n",
      "traversal      4\n",
      "tree           8\n",
      "write          1\n"
     ]
    }
   ],
   "source": [
    "data = data_vect_feat.transpose()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Getting the top 100 frequent words from the manifesto.**\n",
    "\n",
    "We will try to get the top most common 100 words from our document and plot that into a wordcloud for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key Itula, value [('tree', 8), ('binary', 4), ('traversal', 4), ('order', 3), ('following', 3), ('12', 2), ('11', 2), ('avl', 2), ('search', 2), ('preorder', 2), ('ii', 1), ('heap', 1), ('given', 1), ('construc', 1), ('draw', 1), ('insert', 1), ('complete', 1), ('iii', 1), ('10', 1), ('iv', 1), ('keys', 1), ('candidate', 1), ('numbers', 1), ('page', 1), ('post', 1), ('recursion', 1), ('required', 1), ('seat', 1), ('sort', 1), ('level', 1), ('balance', 1), ('bst', 1), ('attempt', 1), ('15', 1), ('16', 1), ('18', 1), ('22', 1), ('23', 1), ('32', 1), ('33', 1), ('34', 1), ('39', 1), ('41', 1), ('45', 1), ('54', 1), ('55', 1), ('67', 1), ('68', 1), ('77', 1), ('85', 1), ('89', 1), ('96', 1), ('advantage', 1), ('apply', 1), ('arrange', 1), ('ascending', 1), ('write', 1)] \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# Find the top 1000 words written in the manifesto\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "    \n",
    "for x in list(top_dict)[0:1000]:\n",
    "    print(\"key {}, value {} \".format(x,  top_dict[x]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tree', 'binary', 'traversal', 'order', 'following', '12', '11', 'avl', 'search', 'preorder']\n"
     ]
    }
   ],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 100 words for each comedian\n",
    "words = []\n",
    "for president in data:\n",
    "    top = [word for (word, count) in top_dict[president]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one array required as input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daves\\OneDrive\\Desktop\\Sem 6\\Research\\test_sub1.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train, X_test , y_train, y_test \u001b[39m=\u001b[39m train_test_split(test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,random_state\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2419\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2417\u001b[0m n_arrays \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(arrays)\n\u001b[0;32m   2418\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2419\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2421\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2423\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: At least one array required as input"
     ]
    }
   ],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daves\\OneDrive\\Desktop\\Sem 6\\Research\\test_sub1.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfidf_vectorizer \u001b[39m=\u001b[39m TfidfVectorizer() \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tfidf_train_vectors \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X63sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tfidf_test_vectors \u001b[39m=\u001b[39m tfidf_vectorizer\u001b[39m.\u001b[39mtransform(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daves\\OneDrive\\Desktop\\Sem 6\\Research\\test_sub1.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwordcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m WordCloud, STOPWORDS\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X64sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimageio\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daves/OneDrive/Desktop/Sem%206/Research/test_sub1.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Image used in which our world cloud output will be\n",
    "img1 = imageio.imread(\"../input/manifesto/itula.jpeg\")\n",
    "hcmask1 = img1\n",
    "\n",
    "# Get 100 words based on the \n",
    "words_except_stop_dist = nltk.FreqDist(w for w in words[:100]) \n",
    "wordcloud = WordCloud(stopwords=set(STOPWORDS),background_color='black',mask=hcmask1).generate(\" \".join(words_except_stop_dist))\n",
    "plt.imshow(wordcloud,interpolation = 'bilinear')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,12)\n",
    "plt.axis('off')\n",
    "plt.title(\"Top most common 100 words from Dr. Itula's Manifesto 2019\",fontsize=20)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('Manifesto_top_100.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Sentiment Analysis of the Manifesto**\n",
    "\n",
    "This is a set of Natural Language Processing (NLP) technique of analysing, identifying and categorizing opinions expressed in a piece of text, in order to determine whether the writer's attitude towards a particular topic, product, politics, services, brands etc. is positive, negative, or neutral. This data holds immense value in the fields of marketing analysis, public relations, product reviews, net promoter scoring, product feedback, and customer service, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets decide  which model we should use, between **TextBlob** and **VADER** for analysis of our text. We will therefore use TextBlob for its simplcity, and since VADER is specifically for analysis of social media data.  \n",
    "\n",
    "#### **7.1 TextBlob function - returns two properties**\n",
    "\n",
    "**Polarity:** a float value which ranges from [-1.0 to 1.0] where 0 indicates neutral, +1 indicates most positive statement and -1 rindicates  most negative statement.\n",
    "\n",
    "**Subjectivity:** a float value which ranges from [0.0 to 1.0] where 0.0 is most objective while 1.0 is most subjective. Subjective sentence expresses some personal opinios, views, beliefs, emotions, allegations, desires, beliefs, suspicions, and speculations where as objective refers to factual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(clean_data)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the polarity is **0.07** which means that the document is **neutral** and **0.47** subjectivity refers almost factual information in the document rather than public opinions, beliefs and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Text Summarization of the Document**\n",
    "\n",
    "![title](https://miro.medium.com/max/1200/1*GIVviyN9Q0cqObcy-q-juQ.png)\n",
    "\n",
    "\n",
    "Text summarization is an important NLP task which is a way of producing a concise and fluent summary of a perticular textin an article, journal, book, comment review, etc while also preseving the key information and overall meaning. Its is divided into categories, i.e., **extraction** and **abstraction**. Extractive methods select a subset of existing words, phrases, or sentences in the original text to form a summary. In contrast, abstractive methods first build an internal semantic representation and then use natural language generation techniques to create a summary.\n",
    "\n",
    "\n",
    "Build a similarity matrix $\\rightarrow$  generate rank based on matrix  $\\rightarrow$ pick top N sentences for summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vectors between two sntences and calculate the cosine angel between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "# One out of 5 words differ => 0.8 similarity\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n",
    " \n",
    "# One out of 2 non-stop words differ => 0.5 similarity\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n",
    " \n",
    "# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n",
    " \n",
    "# Completely different sentences=> 0.0\n",
    "print(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **unsupervised machine learning** approach to find the sentences similarity and rank them using the **cosine similarity** approach. Cosine simirality measures and calculates the similarity between two non-zero vectors of an inner product space by measuring the cosine angle between them.  One benefit of the **cosine similarity** approach is, there;s no need to train and build a model prior start using it for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sentences)\n",
    "\n",
    "# get the english list of stopwords\n",
    "#stop_words = stopwords.words('english')\n",
    " \n",
    "def build_similarity_matrix(lower_case, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros([len(lower_case), len(lower_case)])\n",
    " \n",
    " \n",
    "    for idx1 in range(len(lower_case)):\n",
    "        for idx2 in range(len(lower_case)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(lower_case[idx1], lower_case[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(sentences)\n",
    "#S = build_similarity_matrix(sentences, stop_words)    \n",
    "#S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a function to genrate our summary of the whole document. Also note we will be calling other helper function to keep our summarization pipeline going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(lower_case, top_n=5):\n",
    "    # Remove all the stopwords in the document\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Read text and tokenize\n",
    "    #lower_case  = nltk.word_tokenize(lower_case)\n",
    "    \n",
    "   \n",
    "    \n",
    "    #Generate similarity matrix across sentences\n",
    "    sentence_similarity  = build_similarity_matrix((lower_case, stop_words))\n",
    "    \n",
    "    #Rank sentences in similarity matrix\n",
    "    sentence_similiraty_graph = nx.from_numpy_array(sentence_similarity)\n",
    "    scores = nx.pagerank(sentence_similiraty_graph)\n",
    "    \n",
    "    \n",
    "    #Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(lower_case)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence) \n",
    "    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(' '.join(ranked_sentence[i][1]))\n",
    "        \n",
    "    #Output the summarized text\n",
    "    print('Summarized Text: \\n', '. '.join(summarize_text))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_summary(lower_case, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Been trying to implement the tetxrank algorithm, however we keep getting an error whenever we call the `generate_summary()` function. Feel free to clarify on the error in order to generate a summary using this algorithm. With that said we found a simplest way to do it using just one line of code as shown in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.1 Gensim Package**\n",
    "\n",
    "\n",
    "Lets summarize our text sing the `gensim` package which is a  module for topic modelling for humans.  Summarizing is based on ranks of text sentences using a variation of the TextRank algorithm. see online documentation [gensim](https://radimrehurek.com/gensim/summarization/summariser.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out our summarized text of the document which was converted to lower case, remember we could have opted to remove stopwords as well.\n",
    "\n",
    "print(summarize(lower_case))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This was our summarized text from the document. Note: there are various summarization packages like `TextTeaser`, `PyTeaser` , etc which one can use to accomplish this with only one line of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Topic modeling using LDA** \n",
    "\n",
    "Topic modeling can be seen as a type of statistical modeling for discovering the abstract 'topics' that are presented in a myriad of documents (it can be a single document). A topic  is considered as a collection of prevalent keywords that are typical representatives. Its through keywords in which one determines what the topic is all about.\n",
    "\n",
    "### What is LDA?\n",
    "$\\rightarrow$ Latent Dirichlet Allocation(LDA) is a popular algorithm for topic modeling with excellent implementations in the Python’s `gensim`package. We will therefore use LDA  to classify text in our document to a particular topic. I works by building a topic per document model and words per topic model, from Dirichlet distribution models in statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import graphlab as gl\n",
    "#import pyLDAvis.graphlab\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a dictionary and corpus needed for topic modeling which are the two crucial inputs in implementint the LDA topc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data  = []\n",
    "data.append(clean_text(lower_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we use spacy for lemmatizarion \n",
    "import spacy\n",
    "\n",
    "# Second lemmatization of our data\n",
    "def lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_output = []\n",
    "    for sent in data:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_output\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Lemmatize keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of word id and word frequency in the document, i.e. [0,1] word_id 0 means the word id appers first in the document and word frequency it appears once in the document. For Human readable formart see `cell [87-89]` or else run the following code `[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Building the topic model with LDA\n",
    "\n",
    "\n",
    "Apart from that, `alpha` and `beta` are hyperparameters that affect sparsity of the topics. Both with default values to 1.0/num_topics prior.\n",
    "\n",
    "`alpha` is the per-document topic distribution, in simple terms it is a matrix where each row is a document and each column is a topic.\n",
    "`beta` is the per-topic word distribution, also in simple terms it is a matrix where each row represents a topic and each column represents a word. \n",
    "\n",
    "\n",
    "Also see online documentation for `gensim.gensim.models.ldamodel.LdaModel()` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, alpha='auto', num_topics=20, random_state=100,\n",
    "                                           update_every=1, passes=20, per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets view the topics in our model\n",
    "print(lda_model.print_topics())\n",
    "doc_lda  = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we make sense of this?\n",
    "\n",
    "For topic `0` which is intrepreted as `'0.001*\"need\" + 0.001*\"people\" + 0.001*\"namibian\" + 0.001*\"government\" + 0.001*\"citizen\" + 0.001*\"youth\" + 0.001*\"benefit\" + 0.001*\"system\" + 0.001*\"provide\" + 0.001*\"make\"'`\n",
    "\n",
    "**What does it mean?**  it means that the top 10 keywords that are part of this topic are: `need`, `people`, `namibian`, `government`, `citizen`, `youth`, `benefit`, `system`, `provide` and `make`. The numbers before the words represent the weight of the specific word on that topic, example `need` in topic `0` weigh `0.001` and this is much for all the words top 10 words in topic `0`. \n",
    "\n",
    "\n",
    "Can we deduced what topic this could be by looking on the top 10 keyswords? like what topic could trigger one to talk about `need`, `people`, and so on? We can perhaps summarize it to **POLITICS - Namibian People**\n",
    "\n",
    "This can be done for all the remaining topics to see wether we can come up with a close judgement of each topic.\n",
    "\n",
    "\n",
    "![titile](https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Model Perplexity and Coherence Score\n",
    "\n",
    "\n",
    "Lets evaluate our model by computing the perplexity and topic coherence. Before that we need to understang what model perplexity and coherence is.\n",
    "**Perplexity** is an evaluation  metric on how probable (predictive likelihood) new unseen data is given the model that was learned earlier. **Topic coherence** measures a score on a single topic through measuring the degree of semantic similarity of all the high scoring words in a topic. Both model perplexity and topic score provide a convenient measure to judge how good a given topic model is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model perplexity\n",
    "print('\\nPerplexity:', lda_model.log_perplexity(corpus))\n",
    "\n",
    "\n",
    "# Coherence Score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score:', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The coherance score is 0.27 and perpelexity is -7.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Visualize topic's keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_topics = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we deduce from graph output?\n",
    "\n",
    "On the left it shows a buble in which it represnt a topic, also note we are supposed to have 20 bubles since we chose to have 20 topics in our model earlier on. The larger the buble, the more dominant that topic is. \n",
    "\n",
    "\n",
    "To determine a good topic, the bubble on the left would be big and non-overlapping throught the chart and being in spread along all the quadrants in stead of being clustered. Also note we only have one big bubble which means that we classified our topic 1 very well, however if you look closely you will see a black dot which seem to have all the remaining topic clustered together and overlaping each other in one place.\n",
    "\n",
    "\n",
    "\n",
    "If you move the cursor on the bubble, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic, i.e, top 30 most relevant words. Clicking the  `Previous Topic` and `Next Topic`  buttons on the upper left, we again see the words on the right-hand side updating. However, there isn't much significant changes in most of the words in all the topics as they are all dominated by words like `need`, `youth`, `people`, `government`, `country` , `namibian` and so on... We can say \n",
    "\n",
    "With the output, we can say that the topics in our document exacerbate around **politics**, **Namibian politics** to be specific. Thus we have successfully build a good looking topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. Conclusion**\n",
    "\n",
    "\n",
    "* We have succefully analysed the document eventhough there are many models that can be use to throughly get an idea of the whole document.\n",
    "\n",
    "* The data cleaning process had issues in which there we characters that were not english and thus have tempered with our analysis, this can be improved.\n",
    "\n",
    "* Getting the top 100 most common words resonate well with me on a personal level looking at the state in which Namibia is and what that is needed to be done. This could be because of the word **youth** since I happen fall under that group, haha :-D\n",
    "\n",
    "* For sentiment analysis, we found the document to be neutral with almost factual information in the document rather than public opinions, beliefs and so forth.\n",
    "\n",
    "* The summarization of the document was quite challenging using the TextRank algorimth together with the implementaion of the similarity matrix, however it was easily done with the `gensim` package in one line of code, CAN YOU IMAGINE? Me neither :-;\n",
    "\n",
    "* Topic modelling  was successful, in which we could say that the document is focused on Namibian politics. \n",
    "\n",
    "**Hope you enjoyed reading this. I would appreciate if you leave your thoughts in the comments section. And dont forget to upvote if you liked it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c19fa61d258bb2b35aae2ada233c33e2817c1ce895aa48acba720c6bf7cbe3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
